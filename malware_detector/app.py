import configparser
import datetime
import hashlib
import json  # Added
import logging
import os
import shutil
import tempfile  # Added
import time  # Added for timestamps
import zipfile  # Added

import yara
from flask import Flask, flash, jsonify, redirect, render_template, request, url_for

# --- Basic Logging Setup ---
log_formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)  # Default, overridden by config

# --- Configuration Loading ---
config = configparser.ConfigParser()
BASE_DIR = os.path.abspath(os.path.dirname(__file__))
CONFIG_FILE = os.path.join(BASE_DIR, "config.ini")

DEFAULT_CONFIG = {
    "Paths": {
        "ScanDirectory": "scan_target",
        "QuarantineDirectory": "quarantine",
        "YaraRulesFile": "rules.yar",
        "LogFile": "malware_detector.log",
        "QuarantineMetadataFile": "quarantine/.metadata.json",  # Default metadata location
    },
    "Detection": {
        "SuspiciousExtensions": ".exe,.dll,.bat,.cmd,.vbs,.js,.ps1,.scr,.msi",
        "DoubleExtensionTriggers": ".exe,.bat,.vbs,.scr",
    },
    "Signatures": {"KnownMD5Hashes": "", "KnownSHA256Hashes": ""},
    "Logging": {"LogLevel": "INFO"},
    "Whitelisting": {  # New defaults
        "WhitelistedHashesSHA256": "",
        "WhitelistedFileNames": "",
    },
}

# Read config with defaults
try:
    if not os.path.exists(CONFIG_FILE):
        logger.warning(
            f"Configuration file '{CONFIG_FILE}' not found. Using default settings."
        )
    else:
        config.read(CONFIG_FILE)
        logger.info(f"Loaded configuration from {CONFIG_FILE}")

    # Apply defaults using get methods
    def get_config_val(section, option, fallback):
        return config.get(section, option, fallback=fallback)

    def parse_comma_list(list_string):
        # Ensure case-insensitivity for filenames, consistent case for hashes
        return {item.strip().lower() for item in list_string.split(",") if item.strip()}

    # Paths
    SCAN_DIRECTORY_NAME = get_config_val(
        "Paths", "ScanDirectory", DEFAULT_CONFIG["Paths"]["ScanDirectory"]
    )
    QUARANTINE_DIRECTORY_NAME = get_config_val(
        "Paths", "QuarantineDirectory", DEFAULT_CONFIG["Paths"]["QuarantineDirectory"]
    )
    YARA_RULES_FILE_NAME = get_config_val(
        "Paths", "YaraRulesFile", DEFAULT_CONFIG["Paths"]["YaraRulesFile"]
    )
    LOG_FILE_NAME = get_config_val(
        "Paths", "LogFile", DEFAULT_CONFIG["Paths"]["LogFile"]
    )
    QUARANTINE_METADATA_FILE_NAME = get_config_val(
        "Paths",
        "QuarantineMetadataFile",
        DEFAULT_CONFIG["Paths"]["QuarantineMetadataFile"],
    )

    # Construct full paths
    SCAN_DIRECTORY = os.path.join(BASE_DIR, SCAN_DIRECTORY_NAME)
    QUARANTINE_DIRECTORY = os.path.join(BASE_DIR, QUARANTINE_DIRECTORY_NAME)
    YARA_RULES_FILE = os.path.join(BASE_DIR, YARA_RULES_FILE_NAME)
    LOG_FILE = os.path.join(BASE_DIR, LOG_FILE_NAME)
    # Ensure metadata path is absolute or relative to BASE_DIR if not already absolute
    if not os.path.isabs(QUARANTINE_METADATA_FILE_NAME):
        QUARANTINE_METADATA_FILE = os.path.join(BASE_DIR, QUARANTINE_METADATA_FILE_NAME)
    else:
        QUARANTINE_METADATA_FILE = QUARANTINE_METADATA_FILE_NAME

    # Detection Settings
    SUSPICIOUS_EXTENSIONS = parse_comma_list(
        get_config_val(
            "Detection",
            "SuspiciousExtensions",
            DEFAULT_CONFIG["Detection"]["SuspiciousExtensions"],
        )
    )
    DOUBLE_EXTENSIONS_TRIGGER = parse_comma_list(
        get_config_val(
            "Detection",
            "DoubleExtensionTriggers",
            DEFAULT_CONFIG["Detection"]["DoubleExtensionTriggers"],
        )
    )

    # Signatures
    KNOWN_HASHES = {
        "md5": parse_comma_list(
            get_config_val(
                "Signatures",
                "KnownMD5Hashes",
                DEFAULT_CONFIG["Signatures"]["KnownMD5Hashes"],
            )
        ),
        "sha256": parse_comma_list(
            get_config_val(
                "Signatures",
                "KnownSHA256Hashes",
                DEFAULT_CONFIG["Signatures"]["KnownSHA256Hashes"],
            )
        ),
    }

    # Whitelisting Settings
    WHITELISTED_HASHES_SHA256 = parse_comma_list(
        get_config_val(
            "Whitelisting",
            "WhitelistedHashesSHA256",
            DEFAULT_CONFIG["Whitelisting"]["WhitelistedHashesSHA256"],
        )
    )
    WHITELISTED_FILENAMES = parse_comma_list(
        get_config_val(
            "Whitelisting",
            "WhitelistedFileNames",
            DEFAULT_CONFIG["Whitelisting"]["WhitelistedFileNames"],
        )
    )

    # Logging Level
    LOG_LEVEL_STR = get_config_val(
        "Logging", "LogLevel", DEFAULT_CONFIG["Logging"]["LogLevel"]
    ).upper()
    log_level = getattr(logging, LOG_LEVEL_STR, logging.INFO)
    logger.setLevel(log_level)

    # Setup File Logging Handler
    try:
        # Ensure log directory exists
        log_dir = os.path.dirname(LOG_FILE)
        if log_dir and not os.path.exists(log_dir):
            os.makedirs(log_dir)
        file_handler = logging.FileHandler(LOG_FILE)
        file_handler.setFormatter(log_formatter)
        logger.addHandler(file_handler)
        logger.info(
            f"Logging initialized. Level: {LOG_LEVEL_STR}. Log file: {LOG_FILE}"
        )
    except Exception as e:
        print(f"Error setting up file logger for {LOG_FILE}: {e}")
        logger.error(f"Failed to setup file logging for {LOG_FILE}", exc_info=True)

except Exception as e:
    # Fallback if config reading fails completely
    print(
        f"CRITICAL ERROR during configuration processing: {e}. Using hardcoded defaults."
    )
    logger.critical(
        f"Configuration processing failed: {e}. Using hardcoded defaults.",
        exc_info=True,
    )
    SCAN_DIRECTORY = os.path.join(BASE_DIR, "scan_target")
    QUARANTINE_DIRECTORY = os.path.join(BASE_DIR, "quarantine")
    YARA_RULES_FILE = os.path.join(BASE_DIR, "rules.yar")
    LOG_FILE = os.path.join(BASE_DIR, "malware_detector.log")
    QUARANTINE_METADATA_FILE = os.path.join(BASE_DIR, "quarantine/.metadata.json")
    SUSPICIOUS_EXTENSIONS = {
        ".exe",
        ".dll",
        ".bat",
        ".cmd",
        ".vbs",
        ".js",
        ".ps1",
        ".scr",
        ".msi",
    }
    DOUBLE_EXTENSIONS_TRIGGER = {".exe", ".bat", ".vbs", ".scr"}
    KNOWN_HASHES = {"md5": set(), "sha256": set()}
    WHITELISTED_HASHES_SHA256 = set()
    WHITELISTED_FILENAMES = set()

# --- Initialize Flask App ---
app = Flask(__name__)
app.secret_key = os.environ.get(
    "FLASK_SECRET_KEY", "a_very_insecure_default_key_change_me"
)  # CHANGE THIS default

# --- Load YARA rules ---
try:
    if not os.path.exists(YARA_RULES_FILE):
        logger.warning(
            f"YARA rules file not found at {YARA_RULES_FILE}. YARA detection disabled."
        )
        yara_rules = None
    else:
        yara_rules = yara.compile(filepath=YARA_RULES_FILE)
        logger.info(f"Successfully compiled YARA rules from {YARA_RULES_FILE}")
except yara.Error as e:
    logger.error(
        f"Could not compile YARA rules from {YARA_RULES_FILE}: {e}", exc_info=True
    )
    logger.warning("YARA detection will be disabled.")
    yara_rules = None
# --- End Configuration and Setup ---


# --- Quarantine Metadata Functions ---
def load_q_metadata():
    """Loads quarantine metadata from the JSON file."""
    try:
        if os.path.exists(QUARANTINE_METADATA_FILE):
            with open(QUARANTINE_METADATA_FILE, "r") as f:
                try:
                    return json.load(f)
                except json.JSONDecodeError:
                    logger.error(
                        f"Error decoding JSON from metadata file: {QUARANTINE_METADATA_FILE}"
                    )
                    return {}  # Return empty if corrupt
        else:
            return {}  # Return empty if file doesn't exist
    except Exception as e:
        logger.error(f"Error loading quarantine metadata: {e}", exc_info=True)
        return {}


def save_q_metadata(metadata):
    """Saves quarantine metadata to the JSON file."""
    try:
        # Ensure quarantine directory exists for metadata file
        meta_dir = os.path.dirname(QUARANTINE_METADATA_FILE)
        if meta_dir and not os.path.exists(meta_dir):
            os.makedirs(meta_dir, exist_ok=True)
            logger.info(f"Created directory for metadata file: {meta_dir}")

        with open(QUARANTINE_METADATA_FILE, "w") as f:
            json.dump(metadata, f, indent=4)
        logger.debug(f"Saved quarantine metadata to {QUARANTINE_METADATA_FILE}")
        return True
    except Exception as e:
        logger.error(f"Error saving quarantine metadata: {e}", exc_info=True)
        return False


# --- Core Functions ---
def get_file_hashes(filepath):
    """Calculates MD5 and SHA256 hash of a file."""
    md5_hash = hashlib.md5()
    sha256_hash = hashlib.sha256()
    try:
        with open(filepath, "rb") as f:
            while True:
                data = f.read(4096)
                if not data:
                    break
                md5_hash.update(data)
                sha256_hash.update(data)
        return {"md5": md5_hash.hexdigest(), "sha256": sha256_hash.hexdigest()}
    except Exception as e:
        logger.error(f"Error hashing file {filepath}: {e}", exc_info=True)
        return None


def check_suspicious_extension(filename):
    """Checks for suspicious single or double extensions."""
    # Uses config-loaded sets
    parts = filename.lower().split(".")
    if len(parts) > 1:
        ext = f".{parts[-1]}"
        if ext in SUSPICIOUS_EXTENSIONS:
            return True, f"Suspicious Extension ({ext})"
        if len(parts) > 2:
            double_ext_check = f".{parts[-1]}"
            if double_ext_check in DOUBLE_EXTENSIONS_TRIGGER:
                potential_hidden_ext = f".{parts[-2]}"
                common_decoys = {
                    ".txt",
                    ".pdf",
                    ".doc",
                    ".docx",
                    ".xls",
                    ".xlsx",
                    ".ppt",
                    ".pptx",
                    ".jpg",
                    ".jpeg",
                    ".png",
                    ".gif",
                }
                if potential_hidden_ext in common_decoys:
                    return (
                        True,
                        f"Potential Double Extension ({potential_hidden_ext}{double_ext_check})",
                    )
    return False, ""


def is_whitelisted(filepath, file_hashes):
    """Checks if a file is whitelisted by hash or name."""
    filename_lower = os.path.basename(filepath).lower()
    if filename_lower in WHITELISTED_FILENAMES:
        logger.info(f"Ignoring file due to filename whitelist: {filename_lower}")
        return True

    if file_hashes and file_hashes.get("sha256") in WHITELISTED_HASHES_SHA256:
        logger.info(
            f"Ignoring file due to SHA256 hash whitelist: {filepath} (Hash: {file_hashes['sha256'][:12]}...)"
        )
        return True

    return False


def _scan_single_file(filepath, origin_zip_path=None):
    """Performs all checks on a single file, returns detection dict or None."""
    detection_info = None
    detected = False
    filename = os.path.basename(filepath)
    display_path = f"{origin_zip_path}::>{filename}" if origin_zip_path else filepath

    file_hashes = get_file_hashes(filepath)

    # 0. Whitelist Check (Filename first, then hash)
    if is_whitelisted(filepath, file_hashes):
        return None  # Skip all other checks if whitelisted

    # 1. YARA Rule Matching
    if yara_rules and not detected:
        try:
            matches = yara_rules.match(filepath)
            if matches:
                rule_names = ", ".join([match.rule for match in matches])
                detection_info = {
                    "path": display_path,
                    "reason": "YARA Rule",
                    "details": rule_names,
                    "real_path": filepath,
                }
                detected = True
                logger.warning(f"YARA Match: {rule_names} in file {display_path}")
        except Exception as e:
            logger.error(f"YARA error scanning {display_path}: {e}", exc_info=True)
            # Append to errors list? Need to pass errors list down or handle differently. For now, just log.

    # 2. Hash Matching (MD5 and SHA256)
    if not detected and file_hashes:
        md5 = file_hashes["md5"]
        sha256 = file_hashes["sha256"]
        if md5 in KNOWN_HASHES.get("md5", set()):
            detection_info = {
                "path": display_path,
                "reason": "Known Hash (MD5)",
                "details": md5[:12] + "...",
                "real_path": filepath,
            }
            detected = True
            logger.warning(f"MD5 Match: {md5} in file {display_path}")
        elif sha256 in KNOWN_HASHES.get("sha256", set()):
            detection_info = {
                "path": display_path,
                "reason": "Known Hash (SHA256)",
                "details": sha256[:12] + "...",
                "real_path": filepath,
            }
            detected = True
            logger.warning(f"SHA256 Match: {sha256} in file {display_path}")

    # 3. Suspicious Extension
    if not detected:
        is_suspicious, reason_detail = check_suspicious_extension(filename)
        if is_suspicious:
            detection_info = {
                "path": display_path,
                "reason": "Suspicious Name",
                "details": reason_detail,
                "real_path": filepath,
            }
            detected = True
            logger.info(
                f"Suspicious Name Match: {reason_detail} for file {display_path}"
            )

    return detection_info


def scan_directory(directory):
    """Scans a directory, including ZIP files. If threats are found inside a ZIP,
    flags the ZIP file itself."""
    detected_threats = []
    scanned_files_count = 0
    scanned_archives_count = 0
    errors = []
    logger.info(f"Starting scan in directory: {directory}")

    if not os.path.isdir(directory):
        errmsg = f"Scan directory '{directory}' not found or is not a directory."
        errors.append(errmsg)
        logger.error(errmsg)
        return [], 0, 0, errors

    try:
        for entry in os.scandir(directory):
            filepath = entry.path
            filename = entry.name

            if entry.is_file():
                scanned_files_count += 1
                logger.debug(f"Scanning file: {filepath}")

                # --- ZIP File Handling ---
                if filename.lower().endswith(".zip") and zipfile.is_zipfile(filepath):
                    scanned_archives_count += 1
                    logger.info(f"Scanning archive: {filepath}")
                    archive_contains_threat = False
                    internal_threats_summary = []  # List details of internal threats

                    try:
                        with zipfile.ZipFile(filepath, "r") as zip_ref:
                            with tempfile.TemporaryDirectory() as temp_dir:
                                logger.debug(
                                    f"Extracting '{filename}' to temporary directory: {temp_dir}"
                                )
                                for member in zip_ref.infolist():
                                    if member.is_dir():
                                        continue

                                    # Avoid extracting likely zip bombs (add size checks if needed)

                                    try:
                                        extracted_path = zip_ref.extract(
                                            member, path=temp_dir
                                        )
                                        # Scan the extracted file, but don't add its result directly
                                        detection = _scan_single_file(
                                            extracted_path, origin_zip_path=filepath
                                        )  # Pass origin for logging/display path if needed by _scan_single_file

                                        if detection:
                                            archive_contains_threat = True
                                            # Store summary of internal threat
                                            internal_threats_summary.append(
                                                f"{member.filename} ({detection.get('reason', 'Unknown')})"
                                            )
                                            logger.warning(
                                                f"Threat found inside archive '{filepath}': {member.filename} - Reason: {detection.get('reason','?')}, Details: {detection.get('details','?')}"
                                            )
                                            # Optimization: could break loop here if only need to know *if* threat exists

                                    except zipfile.BadZipFile:
                                        logger.error(
                                            f"Bad zip file encountered when extracting member {member.filename} from {filepath}",
                                            exc_info=True,
                                        )
                                        errors.append(
                                            f"Corrupt member '{member.filename}' in archive '{filename}'"
                                        )
                                    except Exception as extract_err:
                                        logger.error(
                                            f"Error extracting/scanning member {member.filename} from {filepath}: {extract_err}",
                                            exc_info=True,
                                        )
                                        errors.append(
                                            f"Error processing member '{member.filename}' in archive '{filename}'"
                                        )

                        # --- After scanning ZIP: If threats found inside, flag the ZIP file ---
                        if archive_contains_threat:
                            # Check if the archive itself is whitelisted before flagging it
                            zip_hashes = get_file_hashes(
                                filepath
                            )  # Get hash of the zip file itself
                            if not is_whitelisted(filepath, zip_hashes):
                                summary_str = ", ".join(internal_threats_summary)
                                if len(summary_str) > 100:  # Limit details length
                                    summary_str = summary_str[:97] + "..."
                                detected_threats.append(
                                    {
                                        "path": filepath,  # Display path is the zip file path
                                        "reason": "Contained Threats",  # New reason
                                        "details": f"Internal threats: {summary_str}",
                                        "real_path": filepath,  # Actionable path is the zip file path
                                    }
                                )
                                logger.warning(
                                    f"Flagging archive '{filepath}' because it contained threats."
                                )
                            else:
                                logger.info(
                                    f"Archive '{filepath}' contained threats, but archive itself is whitelisted. Ignoring."
                                )

                    except zipfile.BadZipFile:
                        logger.error(
                            f"Cannot open potentially corrupt zip file: {filepath}",
                            exc_info=True,
                        )
                        errors.append(
                            f"Could not open archive '{filename}'. It may be corrupt or invalid."
                        )
                    except Exception as zip_err:
                        logger.error(
                            f"Error processing zip file {filepath}: {zip_err}",
                            exc_info=True,
                        )
                        errors.append(f"Error processing archive '{filename}'")
                # --- End ZIP File Handling ---

                else:  # --- Regular File Handling ---
                    detection = _scan_single_file(filepath)
                    if detection:
                        # 'real_path' is already correct for non-zip files
                        detected_threats.append(detection)
                # --- End Regular File Handling ---

            elif entry.is_dir():
                logger.debug(f"Entering subdirectory: {entry.path}")
                # Recursive call
                sub_threats, sub_scanned, sub_arch_scanned, sub_errors = scan_directory(
                    entry.path
                )
                detected_threats.extend(sub_threats)
                scanned_files_count += sub_scanned
                scanned_archives_count += sub_arch_scanned
                errors.extend(sub_errors)

    except OSError as e:
        errmsg = f"OS Error scanning directory {directory}: {e}"
        errors.append(errmsg)
        logger.error(errmsg, exc_info=True)
    except Exception as e:
        errmsg = f"Unexpected error scanning directory {directory}: {e}"
        errors.append(errmsg)
        logger.error(errmsg, exc_info=True)

    logger.info(
        f"Scan finished for directory: {directory}. Files scanned: {scanned_files_count}. Archives scanned: {scanned_archives_count}. Threats flagged: {len(detected_threats)}. Errors: {len(errors)}."
    )
    return detected_threats, scanned_files_count, scanned_archives_count, errors


# --- Feature 2: Quarantine ---
def quarantine_files(file_paths_details):  # Now expects list of detection dicts
    """Quarantines files and records metadata."""
    quarantined_details = []
    errors = []
    success_count = 0
    metadata = load_q_metadata()
    logger.info(
        f"Attempting to quarantine {len(file_paths_details)} file(s). Target dir: {QUARANTINE_DIRECTORY}"
    )

    if not file_paths_details:
        logger.warning("Quarantine called with empty file list.")
        return [], errors, success_count

    if not os.path.exists(QUARANTINE_DIRECTORY):
        try:
            os.makedirs(QUARANTINE_DIRECTORY, exist_ok=True)
            logger.info(f"Created quarantine directory: {QUARANTINE_DIRECTORY}")
        except OSError as e:
            errmsg = f"Fatal: Could not create quarantine directory '{QUARANTINE_DIRECTORY}': {e}"
            errors.append(errmsg)
            logger.critical(errmsg, exc_info=True)
            return [], errors, success_count

    for item in file_paths_details:
        file_path = item.get("real_path")  # The actual path on disk
        display_path = item.get(
            "path", file_path
        )  # Path shown in UI (might include zip origin)
        reason = item.get("reason", "Unknown")
        details = item.get("details", "")

        if not isinstance(file_path, str) or not file_path:
            errmsg = f"Invalid real file path received for quarantine: {file_path} (Display Path: {display_path})"
            errors.append(errmsg)
            logger.error(errmsg)
            continue

        filename = os.path.basename(file_path)

        if not os.path.exists(file_path):
            errmsg = f"File not found for quarantine: {filename} (Path: {file_path})"
            errors.append(errmsg)
            logger.warning(errmsg)
            continue
        try:
            base, ext = os.path.splitext(filename)
            counter = 0
            # Sanitize display_path slightly for use in filename if needed, or base purely on original filename
            safe_base_name = filename  # Use original filename for destination
            destination = os.path.join(QUARANTINE_DIRECTORY, safe_base_name)

            while os.path.exists(destination):
                counter += 1
                destination = os.path.join(
                    QUARANTINE_DIRECTORY,
                    f"{os.path.splitext(safe_base_name)[0]}_{counter}{os.path.splitext(safe_base_name)[1]}",
                )
                logger.debug(
                    f"Quarantine name conflict for {safe_base_name}, trying {os.path.basename(destination)}"
                )

            shutil.move(file_path, destination)

            # Add to metadata - use destination path as key? Or generate UUID? Using dest path is simpler for now.
            metadata_key = destination  # Use the path in quarantine as the key
            metadata[metadata_key] = {
                "original_path": display_path,  # Show path potentially including zip origin
                "quarantine_path": destination,
                "quarantined_at": datetime.datetime.now().isoformat(),
                "reason": reason,
                "details": details,
            }
            quarantined_details.append(
                metadata[metadata_key]
            )  # Add details for immediate feedback
            success_count += 1
            logger.info(
                f"Successfully quarantined '{filename}' from '{file_path}' to '{destination}'"
            )
        except Exception as e:
            errmsg = f"Error quarantining {filename}: {e}"
            errors.append(errmsg)
            logger.error(errmsg, exc_info=True)

    if success_count > 0:
        if not save_q_metadata(metadata):
            errors.append("Failed to save quarantine metadata file after quarantining.")
            # Maybe try to restore moved files? Complex. Flash warning instead.
            flash(
                "CRITICAL WARNING: Files quarantined but failed to save metadata!",
                "danger",
            )

    logger.info(
        f"Quarantine process finished. Success: {success_count}, Errors: {len(errors)}."
    )
    return quarantined_details, errors, success_count


# Add near other helper functions like remove_files


def restore_files(quarantine_paths_to_restore):
    """Restores selected files from quarantine to their original locations."""
    restored_info = []
    errors = []
    success_count = 0
    metadata = load_q_metadata()  # Assumes load_q_metadata exists and works
    metadata_changed = False
    logger.info(
        f"Attempting to restore {len(quarantine_paths_to_restore)} file(s) from quarantine."
    )

    if not quarantine_paths_to_restore:
        logger.warning("Restore called with empty file list.")
        return [], errors, success_count

    quarantine_abs_path = os.path.abspath(QUARANTINE_DIRECTORY)

    for file_path in quarantine_paths_to_restore:
        if not isinstance(file_path, str) or not file_path:
            errmsg = f"Invalid file path type received for restore: {type(file_path)}"
            errors.append(errmsg)
            logger.error(errmsg)
            continue

        filename = os.path.basename(file_path)
        file_abs_path = os.path.abspath(file_path)

        # Security Check 1: Ensure the file is actually in the quarantine directory
        if not file_abs_path.startswith(quarantine_abs_path):
            errmsg = f"Security Alert: Attempted restore outside quarantine denied for: {filename} (Path: {file_path})"
            errors.append(errmsg)
            logger.critical(errmsg)
            continue

        # Check if file exists in quarantine
        if not os.path.exists(file_path):
            errmsg = f"File not found for restore (already restored/deleted?): {filename} (Path: {file_path})"
            errors.append(errmsg)
            logger.warning(errmsg)
            # Attempt to remove metadata anyway if it exists
            if file_path in metadata:
                del metadata[file_path]
                metadata_changed = True
                logger.debug(
                    f"Removed metadata entry for missing file being 'restored': {filename}"
                )
            continue

        # Get original path from metadata
        item_metadata = metadata.get(file_path)
        if not item_metadata or "original_path" not in item_metadata:
            errmsg = f"Cannot restore '{filename}': Original path metadata not found."
            errors.append(errmsg)
            logger.error(errmsg)
            continue

        original_path = item_metadata["original_path"]

        # Security Check 2: Refuse to restore files originally from inside archives
        if "::>" in original_path:
            errmsg = f"Cannot restore '{filename}': Files originally detected inside archives ({original_path}) cannot be restored directly."
            errors.append(errmsg)
            logger.warning(errmsg)
            continue

        # Security Check 3: Prevent overwriting existing files at original location
        if os.path.exists(original_path):
            errmsg = f"Cannot restore '{filename}' to '{original_path}': File already exists at the destination."
            errors.append(errmsg)
            logger.error(errmsg)
            continue

        # Check and create destination directory if needed
        original_dir = os.path.dirname(original_path)
        try:
            if original_dir and not os.path.exists(original_dir):
                os.makedirs(original_dir, exist_ok=True)
                logger.info(
                    f"Created destination directory for restore: {original_dir}"
                )
        except OSError as e:
            errmsg = f"Cannot restore '{filename}': Failed to create destination directory '{original_dir}': {e}"
            errors.append(errmsg)
            logger.error(errmsg, exc_info=True)
            continue

        # Attempt to move the file
        try:
            shutil.move(file_path, original_path)
            success_count += 1
            logger.info(
                f"Successfully restored '{filename}' from '{file_path}' to '{original_path}'"
            )
            restored_info.append(
                {"quarantine_path": file_path, "original_path": original_path}
            )
            # Remove from metadata only on successful move
            if file_path in metadata:
                del metadata[file_path]
                metadata_changed = True
                logger.debug(f"Removed metadata entry for restored file: {filename}")

        except Exception as e:
            errmsg = f"Error restoring {filename} to {original_path}: {e}"
            errors.append(errmsg)
            logger.error(errmsg, exc_info=True)

    # Save metadata if changes were made
    if metadata_changed:
        if not save_q_metadata(metadata):
            errors.append("Failed to save quarantine metadata file after restore.")
            flash(
                "CRITICAL WARNING: Files restored but failed to update metadata!",
                "danger",
            )

    logger.info(
        f"Restore process finished. Success: {success_count}, Errors: {len(errors)}."
    )
    return restored_info, errors, success_count


# --- Feature 3: Removal ---
def remove_files(quarantine_paths_to_remove):
    """Removes files from quarantine and updates metadata."""
    removed_paths_info = []
    errors = []
    success_count = 0
    metadata = load_q_metadata()
    metadata_changed = False
    logger.info(
        f"Attempting to remove {len(quarantine_paths_to_remove)} file(s) from quarantine: {QUARANTINE_DIRECTORY}"
    )

    if not quarantine_paths_to_remove:
        logger.warning("Remove called with empty file list.")
        return [], errors, success_count

    quarantine_abs_path = os.path.abspath(QUARANTINE_DIRECTORY)

    for file_path in quarantine_paths_to_remove:
        if not isinstance(file_path, str) or not file_path:
            errmsg = f"Invalid file path type received for removal: {type(file_path)}"
            errors.append(errmsg)
            logger.error(errmsg)
            continue

        filename = os.path.basename(file_path)
        file_abs_path = os.path.abspath(file_path)

        if not file_abs_path.startswith(quarantine_abs_path):
            errmsg = f"Security Alert: Attempted removal outside quarantine denied for: {filename} (Path: {file_path})"
            errors.append(errmsg)
            logger.critical(errmsg)
            continue

        # Remove from filesystem
        if os.path.exists(file_path):
            try:
                os.remove(file_path)
                success_count += 1
                logger.info(
                    f"Successfully removed quarantined file: {filename} (Path: {file_path})"
                )
                # Store info about successfully removed file for return value
                removed_paths_info.append(
                    {
                        "quarantine_path": file_path,
                        "original_path": metadata.get(file_path, {}).get(
                            "original_path", "N/A"
                        ),
                    }
                )
            except Exception as e:
                errmsg = f"Error removing {filename} from quarantine: {e}"
                errors.append(errmsg)
                logger.error(errmsg, exc_info=True)
                continue  # Don't attempt to remove metadata if file removal failed
        else:
            # File already gone, maybe log warning, still attempt metadata cleanup
            logger.warning(
                f"File not found for removal (already removed?): {filename} (Path: {file_path})"
            )
            # Store info for return value even if file was already gone
            removed_paths_info.append(
                {
                    "quarantine_path": file_path,
                    "original_path": metadata.get(file_path, {}).get(
                        "original_path", "N/A"
                    ),
                }
            )

        # Remove from metadata if it exists
        if file_path in metadata:
            try:
                del metadata[file_path]
                metadata_changed = True
                logger.debug(
                    f"Removed metadata entry for {filename} (Path: {file_path})"
                )
            except Exception as e:
                # Should not happen if key exists, but log just in case
                logger.error(
                    f"Error removing metadata entry for {file_path}: {e}", exc_info=True
                )

    # Save metadata if changes were made
    if metadata_changed:
        if not save_q_metadata(metadata):
            errors.append("Failed to save quarantine metadata file after removal.")
            flash(
                "CRITICAL WARNING: Files removed but failed to update metadata!",
                "danger",
            )

    logger.info(
        f"Removal process finished. Success/Targeted: {success_count}/{len(quarantine_paths_to_remove)}, Errors: {len(errors)}."
    )
    # Return info about files targeted/processed, including those already gone
    return removed_paths_info, errors, len(quarantine_paths_to_remove)


# --- Flask Routes ---
# Add near other routes


@app.route("/restore", methods=["POST"])
def perform_restore():
    """Handles restoring files from quarantine."""
    current_year = datetime.datetime.now().year
    # These values should be the actual paths *within* the quarantine directory
    quarantine_paths_to_restore = request.form.getlist("file_to_action")
    origin_page = request.form.get(
        "origin", "quarantine_view"
    )  # Should always come from quarantine_view

    logger.info(
        f"Restore request received for {len(quarantine_paths_to_restore)} file(s)."
    )

    if not quarantine_paths_to_restore:
        flash("No files selected for restore.", "warning")
        logger.warning("Restore request received but no files were selected.")
        return redirect(url_for(origin_page))  # Redirect back to quarantine view

    flash(
        f"Attempting to restore {len(quarantine_paths_to_restore)} selected quarantined file(s)...",
        "info",
    )
    restored_info, errors, success_count = restore_files(quarantine_paths_to_restore)

    for error in errors:
        flash(error, "danger")  # Logged within function

    if success_count > 0:
        flash(
            f"Successfully restored {success_count} file(s).", "success"
        )  # Logged within function
    elif errors:
        flash("Failed to restore any selected files. See errors above.", "warning")
    else:
        flash("No files were restored.", "info")

    # Redirect back to the quarantine view page after restore attempt
    return redirect(url_for("quarantine_view"))


@app.route("/")
def index():
    current_year = datetime.datetime.now().year
    logger.debug(f"Serving index page. Scan dir: {SCAN_DIRECTORY_NAME}")
    if not os.path.isdir(SCAN_DIRECTORY):
        flash(
            f"Warning: Scan directory '{SCAN_DIRECTORY_NAME}' not found. Please create it relative to the application.",
            "warning",
        )
        logger.warning(f"Scan directory does not exist: {SCAN_DIRECTORY}")
    return render_template(
        "index.html", scan_dir=SCAN_DIRECTORY_NAME, current_year=current_year
    )


@app.route("/scan", methods=["POST"])
def perform_scan():
    current_year = datetime.datetime.now().year
    logger.info("Scan request received.")
    flash(f"Starting scan of '{SCAN_DIRECTORY_NAME}'...", "info")
    # Get results from scan function
    detected_threats, scanned_files, scanned_archives, scan_errors = scan_directory(
        SCAN_DIRECTORY
    )

    for error in scan_errors:
        flash(error, "danger")

    flash(
        f"Scan complete. Scanned {scanned_files} files and {scanned_archives} archives.",
        "secondary",
    )

    if not detected_threats:
        flash("No threats detected based on current rules and signatures.", "success")
        logger.info("Scan completed, no threats detected.")
    else:
        flash(f"Detected {len(detected_threats)} potential threats.", "warning")
        logger.warning(f"Scan completed, {len(detected_threats)} threats detected.")

    # Pass the list of threat dictionaries (containing 'real_path')
    return render_template(
        "results.html", detected_threats=detected_threats, current_year=current_year
    )


@app.route("/quarantine", methods=["POST"])
def perform_quarantine():
    """Handle the quarantine request."""
    current_year = datetime.datetime.now().year
    quarantine_items_data = []  # This will hold the list of dicts after parsing

    # --- UPDATED: Read the single JSON string ---
    selected_json_str = request.form.get("selected_items_json")  # Get the single field

    if not selected_json_str:
        flash("No selection data received from form.", "warning")
        logger.warning(
            "Quarantine request received but 'selected_items_json' field was missing or empty."
        )
        return redirect(url_for("index"))

    # --- UPDATED: Parse the JSON array string ---
    try:
        items_to_process = json.loads(selected_json_str)
        if not isinstance(items_to_process, list):
            raise ValueError("Parsed JSON is not a list")

        # Basic validation of items within the list
        for item in items_to_process:
            if isinstance(item, dict) and "real_path" in item and "path" in item:
                quarantine_items_data.append(item)
            else:
                logger.warning(
                    f"Skipping invalid item structure found in submitted JSON list: {item}"
                )

    except (json.JSONDecodeError, ValueError) as e:
        logger.error(
            f"Failed to decode or validate JSON data from 'selected_items_json': {e}",
            exc_info=True,
        )
        logger.debug(
            f"Received data: {selected_json_str}"
        )  # Log received data on error
        flash("Error processing selection data. Invalid format received.", "danger")
        # Redirect back to results might be better here if data is corrupt
        return redirect(url_for("index"))  # Or url_for('results')?

    # --- Original logic continues below ---

    if not quarantine_items_data:
        # This check might now indicate all items parsed were invalid, or the list was empty
        flash("No valid files selected or processed for quarantine.", "warning")
        logger.warning(
            "Quarantine processing started but resulted in zero valid items."
        )
        return redirect(url_for("index"))  # Or url_for('results')?

    logger.info(
        f"Quarantine request parsed successfully for {len(quarantine_items_data)} item(s)."
    )
    flash(
        f"Attempting to quarantine {len(quarantine_items_data)} selected item(s)...",
        "info",
    )
    quarantined_details, errors, success_count = quarantine_files(
        quarantine_items_data
    )  # Pass list of dicts

    for error in errors:
        flash(error, "danger")

    if success_count > 0:
        flash(f"Successfully quarantined {success_count} file(s).", "success")

    # Rerender final page (or redirect to new quarantine view?)
    return render_template(
        "final.html",
        action_taken="Quarantine",
        action_details=quarantined_details,
        allow_removal=False,  # Removal is done via quarantine_view
        current_year=current_year,
    )


# --- NEW ROUTE for Quarantine Management ---
@app.route("/quarantine_view")
def quarantine_view():
    """Displays the contents of the quarantine."""
    current_year = datetime.datetime.now().year
    logger.info("Request received for quarantine view page.")
    metadata = load_q_metadata()  # Assumes load_q_metadata handles file not found etc.

    processed_items = []
    # --- Add error handling for individual items ---
    for key, item_data in metadata.items():
        try:
            # Ensure item_data is a dictionary before proceeding
            if not isinstance(item_data, dict):
                logger.warning(
                    f"Skipping non-dictionary item in metadata with key: {key}"
                )
                continue

            # Calculate basename safely
            q_path = item_data.get("quarantine_path")  # Use .get for safety
            if q_path and isinstance(q_path, str):
                item_data["basename"] = os.path.basename(q_path)
            else:
                item_data["basename"] = "N/A - Invalid Path"
                logger.warning(
                    f"Missing or invalid 'quarantine_path' for metadata key: {key}"
                )

            # Ensure essential keys exist for display (or template handles defaults)
            if "original_path" not in item_data:
                item_data["original_path"] = "N/A"
            # Ensure quarantine_path exists, critical for the form value
            if "quarantine_path" not in item_data or not item_data["quarantine_path"]:
                logger.warning(
                    f"Missing/empty quarantine_path for metadata key {key}, setting to placeholder."
                )
                item_data["quarantine_path"] = (
                    f"error_missing_path_{key}"  # Placeholder needed
                )
            if "reason" not in item_data:
                item_data["reason"] = "Unknown"
            if "quarantined_at" not in item_data:
                item_data["quarantined_at"] = ""

            processed_items.append(item_data)

        except Exception as e:
            # Log error for specific item but continue processing others
            logger.error(
                f"Error processing metadata item with key {key}: {e}", exc_info=True
            )
            # Optionally add a placeholder to processed_items to indicate an error occurred

    # Sort the processed items safely using .get with a default
    try:
        quarantined_items = sorted(
            processed_items,
            key=lambda x: x.get(
                "quarantined_at", ""
            ),  # Default empty string if key missing
            reverse=True,
        )
    except TypeError as e:
        logger.error(
            f"Error sorting quarantined items, possibly due to incompatible types for 'quarantined_at': {e}"
        )
        quarantined_items = processed_items  # Render unsorted if sorting fails

    logger.debug(
        f"Rendering quarantine view with {len(quarantined_items)} items after processing."
    )
    return render_template(
        "quarantine_view.html",
        quarantined_items=quarantined_items,
        current_year=current_year,
    )


@app.route("/remove", methods=["POST"])
def perform_removal():
    """Handles removal of files *from quarantine*."""
    current_year = datetime.datetime.now().year
    # These values should be the actual paths *within* the quarantine directory
    quarantine_paths_to_remove = request.form.getlist("file_to_action")
    origin_page = request.form.get(
        "origin", "quarantine_view"
    )  # Hidden input to know where to redirect

    logger.info(
        f"Removal request received for {len(quarantine_paths_to_remove)} file(s) from quarantine."
    )

    if not quarantine_paths_to_remove:
        flash("No files selected for removal.", "warning")
        logger.warning("Removal request received but no files were selected.")
        return redirect(
            url_for(origin_page if origin_page == "quarantine_view" else "index")
        )

    flash(
        f"Attempting to remove {len(quarantine_paths_to_remove)} selected quarantined file(s)...",
        "info",
    )
    removed_paths_info, errors, target_count = remove_files(quarantine_paths_to_remove)
    success_count = len(removed_paths_info)  # Number actually processed/removed

    for error in errors:
        flash(error, "danger")  # Logged within function

    if success_count > 0:
        # Check if success_count matches target_count for more precise message?
        flash(
            f"Successfully processed removal for {success_count} file(s).", "success"
        )  # Logged within function
    elif errors:
        flash("Failed to remove any selected files. See errors above.", "warning")
    else:
        # Should not happen if list was not empty, but handle anyway
        flash("No files were removed.", "info")

    # Redirect back to the quarantine view page after removal
    return redirect(url_for("quarantine_view"))


# --- Main Execution ---
if __name__ == "__main__":
    logger.info("-------------------- Application Starting --------------------")
    if not os.path.exists(SCAN_DIRECTORY):
        logger.warning(
            f"Scan directory '{SCAN_DIRECTORY}' does not exist at startup. Please create it."
        )
    if yara_rules is None:
        logger.warning(
            "YARA rule engine failed to initialize. YARA detection is disabled."
        )

    log_dir = os.path.dirname(LOG_FILE)
    if log_dir and not os.path.exists(log_dir):
        try:
            os.makedirs(log_dir)
        except Exception as e:
            print(f"ERROR: Failed to create log directory '{log_dir}': {e}")
            logger.error(f"Failed to create log directory '{log_dir}'", exc_info=True)

    if log_dir and not os.access(log_dir, os.W_OK):
        print(
            f"WARNING: Log directory '{log_dir}' is not writable. File logging might fail."
        )
        logger.warning(f"Log directory '{log_dir}' is not writable.")

    debug_mode = os.environ.get("FLASK_DEBUG", "0").lower() in ("true", "1", "t")
    logger.info(f"Flask debug mode: {debug_mode}")
    # Consider using waitress or gunicorn for production instead of Flask dev server
    app.run(
        debug=debug_mode, host="0.0.0.0"
    )  # host='0.0.0.0' makes it accessible on network if needed
